---
title: DSLs and Compilers, Part 1
layout: lecture
mathjax: true
---

### Part 1: The Front End

- [The Structure of a Compiler](#intro)
- [Front End](#frontend)
  - [Formal Grammars](#grammars)
  - [Lexical Analysis](#lexer)
  - [Parsing](#parser)
    - [Backus-Naur Form](#bnf)
    - [Ambiguous Grammars](#ambiguous)
    - [Types of Parsers](#types)
  - [Abstract Syntax Trees](#ast)
  - [Semantic Analysis](#semantic_analysis)
    - [Attribute Grammars](#attributes)
    - [AST walking](#walking)


<a name="#intro"></a>
# The Structure of a Compiler

![Compiler Structure]({{site.baseurl}}/lectures/m09/compiler_structure.png)
(adapted from Muchnick,'97)

- "Front" vs "Back"
  - Front end is largely about *representation*.
  - Back end is largely about *function*.

- It's a process of continuous transformation. Layers can, for instance:
  - remove information (simplify)
  - add information (analyze)
  - change representation (translate)
  - modify behavior (optimize)

[Back to top &uarr;](#)

<a name="#frontend"></a>
# Front End

<a name="#grammars"></a>
## Formal Grammars

- Formal grammar consists of three parts:
  - Terminal symbols (constants, atomic symbols)
  - Nonterminal symbols (variables, replaceable symbols)
  - Production rules (equations, transformations)

*In examples, use numbers for terminals, letters for non-terminals*

- Chomsky's formal grammar hierarchy
  - Type 3: "regular", regex's; finite state machines can recognize; rules are applied based on the symbol only
  - Type 2: "context-free", pushdown automata can recognize; rules are applied based on the symbol only, but may continue to be applied
  - Type 1: "context-sensitive", linear-bounded automata can recognize; rules are applied based on the symbol and a finite number of symbols around it, and can continue to be applied
  - Type 0: "recursively enumerable" or "unrestricted", basically anything; Turing machines can recognize; rules can do whatever the hell they want

[Back to top &uarr;](#)

<a name="#lexer"></a>
## Lexical Analysis

- many times, we don't need character-by-character analysis
- instead, group them into chunks called "tokens"
- "lexical analysis"
- goal of a lexer: simplify, so parsers are easier
- typically regular grammar (type 3) rules

[Back to top &uarr;](#)

<a name="#parser"></a>
## Parsing

- goal of a parser: turn tokens into a structured representation of code
- typically context-free grammar (type 2) rules

<a name="#bnf"></a>
### Backus-Naur Form
- Okay, how do we specify a grammar in practice?
- BNF
- LR(1) example grammar in BNF

[Back to top &uarr;](#)

<a name="ambiguous"></a>
### Ambiguous Grammars
- ambiguous grammars
  - what makes a grammar ambiguous?
  - example of conflicting or infinitely-recursive production rules

<a name=#parser_types></a>
### Types of Parsers
- Top-down vs. Bottom-up
  - Top-down (LL) starts with the most general rule and breaks it down
  - Bottom-up (LR) starts with the nothing and combines things

[Back to top &uarr;](#)


<a name="#ast"></a>
## Abstract Syntax Trees

- turns syntactic representation into semantic representation
- typically a reduced set of the concrete syntax tree

[Back to top &uarr;](#)

<a name="#semantic_analysis"></a>
## Semantic Analysis

- Lexing/parsing enforces *syntactic* rules
- Now enforce *semantic* rules

[Back to top &uarr;](#)

<a name="#attributes"></a>
### Attribute Grammars

- why? principled approach to augmenting the AST
  - why not just do it? Inconsistent data. Want to maintain invariants (e.g., types must agree, labels must match, etc.)
- formal definition
- inherited (top-down) vs. synthesized (bottom-up)

[Back to top &uarr;](#)

<a name="#walking"></a>
### AST walking

- most people don't use attribute grammars
  - why? too much effort, not expressive enough
  - BUT: their spirit lives on in structure AST walks.
- write an AST walk that's kind of like an attribute grammar

[Back to top &uarr;](#)
