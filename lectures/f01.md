---
title: Introduction to C
layout: lecture
mathjax: true
---

 - [Computer architecture](#architecture)
 - [Compilers, linkers, and debuggers](#compiler)
 - [Basic C syntax](#syntax)
 - [Pointers](#pointers)
 - [Allocation](#allocation)

<a name='architecture'></a>
## Computer architecture
In its simplest form, your computer looks like this:

![Simple computer]({{site.baseurl}}/lectures/f01/simple_computer_0.png)

Your data is stored in memory, and your CPU executes your programs. When you're surfing the web or writing email, this is a good mental model to use.
Realistically, however, computers are amongst the most complicated devices ever invented, and to understand how programs actually work, we're going to need to go a bit deeper.

Let's start by introducing a program fragment that we can use as a motivating example for talking about different pieces of the computer.
This code is written in C, but it should look familiar enough to any programmer.

```c
...
x = M[0];
y = M[1];
z = x + y;
M[2] = z;
...
```

Processors get work done by executing a series of *instructions*, primitive computational tasks like adding or multiplying two numbers.
These are carried out by a piece of the processor called the arithmetic logic unit, or ALU.
In the earliest days of computing, an ALU would process values directly out of memory, but in modern devices, memory is actually an incredibly long way away relative to the distances inside a CPU.
If an ALU were to compute using data in memory directly, your CPU would be at least 100 times slower than it is now.
Instead, the ALU reads and writes its values to *registers*, which are very small, fast memory locations nearby.

When we run the code fragment above, we can pretend that three basic things happen: first, the values `x`, `y`, and `z` are copied from memory to registers.
Second, the ALU computes the sum and stores it to another register.
Third, the result is copied from a register to memory.
It looks something like this:

![Less simple computer]({{site.baseurl}}/lectures/f01/simple_computer_1.png)

Of course, I said "we can pretend..." because this is all a lie, too.
This is just another, slightly-less-simple model of how computers work.
It's somewhat more accurate, and for some tasks, it's more useful, but it's a long way away from being a complete picture.
In fact, it's difficult to imagine what a complete picture would even look like, because it would involve all five billion transistors.
So in practice, everyone uses a model that is overly simplistic to *some* degree, and the question of how simple depends on the work you'd like to do.

For our purposes, we're going to add a couple more details to our model before we move on.

If you recall from your previous lab, we used a technique called *memoization* to remember the result of function calls.
Memoization is a special form of *caching*.
Caching is a pretty fundamental idea in computer science, and it's used all over.
The basic idea goes like this:

 - Fast memory is usually expensive, and slow memory is usually cheap.
 - Some of your data will be used a lot more often then others.
 - Put frequently-used data in a small, expensive, fast memory.
 - Put rarely-used data in a big, cheap, slow memory.

Modern processors have several types of caches of varying sizes and functions, but they all follow basically the same rules, and they work automatically behind the scenes.
When the ALU needs a piece of data from memory, it is loaded into a register, but a copy is stored in caches along the way.
If we have more values than registers, we have to replace some value.
If we then need to use that value again, instead of going all the way to memory to look it up, we can use the copy stored in cache.

![Even less simple computer]({{site.baseurl}}/lectures/f01/simple_computer_2.png)

This structure of increasingly larger but slower storage is called the *memory hierarchy*, and it is one of the most important things to understand about computer architecture when trying to make your programs fast.

#### Numbers to live by
Registers are "fast", and memory is "slow", but these are relative terms.
What do we really mean?
Every processor is slightly different, but we can give a rough order-of-magnitude.
Modern processors operate at around 2GHz and can run most instructions in a single cycle, which means an add takes about 0.5 nanoseconds.

This idea originally inspired by [Peter Norvig](http://norvig.com/21-days.html#answers) and updated somewhat for 15-year-old numbers.

Task | Approximate latency in log2(cycles) | Same, in cycles
---|---|---
Add two numbers | 0 | 1
Access a register | 0| 1
Access L1 cache [ref](http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf) | 2 | 4
Access L2 cache [ref](http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf) | 4 | 16
Access L3 cache [ref](http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf) | 5 | 32
Access memory | 7 | 128
Access disk [ref](https://en.wikipedia.org/wiki/Hard_disk_drive#Seek_time)| 10 | 1024
Ping a server in LA from NY [ref](https://ipnetwork.bgtmo.ip.att.net/pws/network_delay.html) | 17 | 131072

Take these numbers with a grain of salt: I've rounded them to a close power of two, and they depend heavily on other conditions.
Still, it's useful to keep in mind when estimating the cost of something.

[Back to top &uarr;](#)

<a name='compiler'></a>
## Compilers, linkers, and debuggers
Computers  

<a name='syntax'></a>
## Basic C syntax
Most languages have similar syntax for expressions. `a+b` works just like you would expect in python.

### Static types



<a name='pointers'></a>
## Pointers

<a name='allocation'></a>
## Allocation

### The stack and the heap

### Malloc and free
